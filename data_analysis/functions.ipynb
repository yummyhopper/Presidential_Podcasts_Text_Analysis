{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "304ae352",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import pandas as pd\n",
    "import re\n",
    "import graphviz\n",
    "import networkx as nx\n",
    "from IPython.display import display, HTML\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c1144b77",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_key(fdist1, fdist2, fthreshold=5, keyness_threshold=6.6, top=100, print_table=True):\n",
    "    'create a keyness comparison table from two frequency lists'\n",
    "    \n",
    "    c1size = sum(fdist1.values())\n",
    "    c2size = sum(fdist2.values())\n",
    "    log_val = math.log((c1size+c2size))\n",
    "\n",
    "    kdata = []\n",
    "    for item, freq in fdist1.items():\n",
    "        if freq<fthreshold:\n",
    "            continue  \n",
    "        ref_freq = fdist2.get(item,0)\n",
    "        if ref_freq<fthreshold:\n",
    "            continue\n",
    "        \n",
    "        keyness = log_likelihood(freq, c1size, ref_freq, c2size)\n",
    "        pdiff = (((freq/c1size)-(ref_freq/c2size))*100/(ref_freq/c2size))\n",
    "        bic = ((log_likelihood(freq, c1size, ref_freq, c2size))- log_val)\n",
    "        \n",
    "        row = {'item': item, 'freq': freq, 'ref_freq': ref_freq,'keyness':keyness, 'pdiff':pdiff,'bic': bic}\n",
    "        kdata.append(row)\n",
    "    kdf = pd.DataFrame(kdata)[['item', 'freq', 'ref_freq','keyness','pdiff','bic']]\n",
    "    kdf=kdf.sort_values('keyness', ascending=False)\n",
    "    if not print_table:\n",
    "        return kdf[:top]\n",
    "    template = \"{: <25}{: <10}{: <10}{: <10}{: <10}{:0.3f}\"\n",
    "    header = \"{: <25}{: <10}{: <10}{: <10}{: <10}{}\".format('WORD', 'A Freq.', 'B Freq.','Keyness','%DIFF','BIC')\n",
    "    print(\"{}\\n{}\".format(header, \"=\"*len(header)))\n",
    "    for item, freq, ref_freq, keyness, pdiff, bic in kdf[:top].values:\n",
    "        print(template.format(item, freq, ref_freq, round(keyness,3), round(pdiff,3), bic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d3dfb74b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_keyness(fdist1, fdist2, fthreshold=5, keyness_threshold=6.6, top=100, print_table=True):\n",
    "    '''\n",
    "    '''\n",
    "    \n",
    "    c1size = sum(fdist1.values())\n",
    "    c2size = sum(fdist2.values())\n",
    "\n",
    "    \n",
    "    kdata = []\n",
    "    \n",
    "    for item, freq in fdist1.items():\n",
    "        if freq<fthreshold:\n",
    "            continue\n",
    "            \n",
    "        ref_freq = fdist2.get(item,0)\n",
    "        \n",
    "        if ref_freq<fthreshold:\n",
    "            continue\n",
    "        \n",
    "        \n",
    "        keyness = log_likelihood(freq, c1size, ref_freq, c2size)\n",
    "        \n",
    "        row = {'item': item, 'freq': freq, 'ref_freq': ref_freq, 'keyness': keyness}\n",
    "        \n",
    "        if keyness>keyness_threshold:\n",
    "        \n",
    "            kdata.append(row)\n",
    "        \n",
    "    \n",
    "    kdf = pd.DataFrame(kdata)[['item', 'freq', 'ref_freq', 'keyness']]\n",
    "    \n",
    "    kdf=kdf.sort_values('keyness', ascending=False)\n",
    "    \n",
    "    if not print_table:\n",
    "        return kdf[:top]\n",
    "    \n",
    "    template = \"{: <25}{: <10}{: <10}{:0.3f}\"\n",
    "    \n",
    "    header = \"{: <25}{: <10}{: <10}{}\".format('WORD', 'Corpus A', 'Corpus B', 'Keyness')\n",
    "    \n",
    "    print(\"{}\\n{}\".format(header, \"=\"*len(header)))\n",
    "    \n",
    "    for item, freq, ref_freq, keyness in kdf[:top].values:\n",
    "        print(template.format(item, freq, ref_freq, keyness))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "382a74d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_likelihood(item_A_freq, corpus_A_size, item_B_freq, corpus_B_size):\n",
    "    '''calculate the log likelihood score for a comparison between the frequency of two items\n",
    "    '''\n",
    "    E1 = corpus_A_size*(item_A_freq+item_B_freq) / (corpus_A_size+corpus_B_size)\n",
    "    E2 = corpus_B_size*(item_A_freq+item_B_freq) / (corpus_A_size+corpus_B_size)\n",
    "\n",
    "    G2 = 2*((item_A_freq*math.log(item_A_freq/E1)) + (item_B_freq*math.log(item_B_freq/E2)))\n",
    "    \n",
    "    sign = 1 if (item_A_freq / corpus_A_size) >= (item_B_freq / corpus_B_size) else -1\n",
    "    \n",
    "    return sign*G2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "60a70ae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_keyitems(df, num=10, c1='red', c2='blue', corpusA='corpus A', corpusB='corpus B'):\n",
    "    '''create a horizontal bar plot of top/bottom N items in a keyness table\n",
    "    \n",
    "    Args:\n",
    "        df - a data frame created by calculated_keyness with cols: item, keyness\n",
    "        num - the number of top and bottom ranked items to include\n",
    "        c1/c2 - color for the bars\n",
    "        corpusA/corpusB - labels/names of corpora\n",
    "        \n",
    "    Returns:\n",
    "        matplotlib plot \n",
    "    '''\n",
    "    def selc_df(df, x=2):\n",
    "        return df.head(x).append(df.tail(x))\n",
    "\n",
    "    tb_df=selc_df(df,num)\n",
    "    \n",
    "    yh=int(num/10)*5\n",
    "    \n",
    "    colors = [c1]*num + [c2]*num\n",
    "    \n",
    "    ax = tb_df.set_index('item')['keyness'].plot(kind='barh', zorder=2,\n",
    "                                        figsize=(8, yh),\n",
    "                                        color=colors, alpha=0.5, width=0.75)\n",
    "    \n",
    "    # Despine\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['left'].set_visible(False)\n",
    "    ax.spines['bottom'].set_visible(False)\n",
    "    \n",
    "    \n",
    "    # Draw vertical axis lines\n",
    "    vals = ax.get_xticks()\n",
    "    for tick in vals:\n",
    "        ax.axvline(x=tick, linestyle='dashed', alpha=0.4, color='#eeeeee', zorder=1)\n",
    "\n",
    "        \n",
    "    ax.set_xlabel(\"Keyness\", labelpad=20, weight='bold', size=12)\n",
    "\n",
    "    # Set y-axis label\n",
    "    ax.set_ylabel(\"\")\n",
    "\n",
    "    ax.annotate(f'Distinctive items\\nin {corpusB}', (10,num+num/2), color=c2)\n",
    "    ax.annotate(f'Distinctive items\\nin {corpusA}', (-10,num/2), ha='right', color=c1)\n",
    "\n",
    "    \n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5a4d0dc1-50f4-49e0-bab1-8279edef80e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_keyitems(df, n=20, c1='red', c2='blue', corpusA='corpus A', corpusB='corpus B'):\n",
    "    '''plot  top/bottom n items from a keyness analysis table\n",
    "    \n",
    "    Args:\n",
    "        df - a data frame created by calculated_keyness with cols: item, keyness\n",
    "        num - the number of top and bottom ranked items to include\n",
    "        c1/c2 - color for the bars\n",
    "    \n",
    "    Returns:\n",
    "        HTML string containing two column table\n",
    "    '''\n",
    "   \n",
    "    template = '''\n",
    "        <div style=' float:left; width: 40%; text-align: center'>\n",
    "        <h3>{}</h3>\n",
    "        {}</div>\n",
    "       <div style='width: 40%; padding-left: 20px; float: left; '>\n",
    "       <h3 style=\"text-align: center\">{}</h3>\n",
    "        {}</div>\n",
    "    '''\n",
    "\n",
    "\n",
    "    idiv = '''\n",
    "            <div style=\"font-size: {}px; color: {}; margin-bottom: 2px; float: left; \n",
    "            margin: 10px; padding: 2px; background-color: #f7f7f7; border-radius: 6px\">\n",
    "            {}</div>\n",
    "            '''\n",
    "    \n",
    "    top = df[['item', 'keyness']].head(n).values\n",
    "    bottom = df[['item', 'keyness']].tail(n).values\n",
    "\n",
    "    top_str = '\\n'.join([idiv.format(5*math.log(kness), c1, item) for size, (item, kness) in enumerate(top,1)])\n",
    "    bottom_str = '\\n'.join([idiv.format(5*math.log(abs(kness)), c2, item) for size, (item, kness) in enumerate(bottom,1)])\n",
    "    \n",
    "    \n",
    "    display(HTML(\n",
    "        template.format(corpusA,top_str, corpusB, bottom_str)\n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2728359d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text, lowercase=False, strip_chars=''):\n",
    "    '''create a list of tokens from a string by splitting on whitespace and applying optional normalization \n",
    "    \n",
    "    Args:\n",
    "        text        -- a string object containing the text to be tokenized\n",
    "        lowercase   -- should text string be normalized as lowercase (default: False)\n",
    "        strip_chars -- a string indicating characters to strip out of text, e.g. punctuation (default: empty string) \n",
    "        \n",
    "    Return:\n",
    "        A list of tokens\n",
    "    '''\n",
    "    \n",
    "    # create a replacement dictionary from the\n",
    "    # string of characters in the **strip_chars**\n",
    "    rdict = str.maketrans('','',strip_chars)\n",
    "    \n",
    "    if lowercase:\n",
    "        text = text.lower()\n",
    "    \n",
    "    tokens = text.translate(rdict).split()\n",
    "    \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "be2b0266",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ngram_tokens(tokens, n=1):\n",
    "    '''create a list of n-gram tokens from a list of tokens\n",
    "    \n",
    "    Args:\n",
    "        tokens -- a list of tokens\n",
    "        n      -- the size of the window to use to build n-gram token list\n",
    "        \n",
    "    Returns:\n",
    "        \n",
    "        list of n-gram strings (whitespace separated) of length n\n",
    "    '''\n",
    "    \n",
    "    if n<2 or n>len(tokens):\n",
    "        return tokens\n",
    "    \n",
    "    new_tokens = []\n",
    "    \n",
    "    for i in range(len(tokens)-n+1):\n",
    "        new_tokens.append(\" \".join(tokens[i:i+n]))\n",
    "        \n",
    "    return new_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0a16d761",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_kwic(kw, text, win=4):\n",
    "    '''A basic KWIC function for a text\n",
    "    \n",
    "    Args:\n",
    "        kw   -- string match for keyword to match for each line\n",
    "        text -- a list of tokens for the text\n",
    "        \n",
    "    Return:\n",
    "        list of lines of form [ [left context words], kw, [right context words]]\n",
    "    '''\n",
    "    \n",
    "    hits = [(w,i) for i,w in enumerate(text) if w==kw]\n",
    "    \n",
    "    lines = []\n",
    "    for hit in hits:\n",
    "        left = text[hit[1]-win:hit[1]]\n",
    "        kw = text[hit[1]]\n",
    "        right = text[hit[1]+1 : hit[1]+win+1]\n",
    "        \n",
    "        \n",
    "        left = ['']*(win-len(left)) + left if len(left)<win else left\n",
    "        right = right+['']*(win-len(right)) if len(right)<win else right\n",
    "\n",
    "        \n",
    "        lines.append([left, kw, right])\n",
    "        \n",
    "    return lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "70f436bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_kwic(kwic, win=None):\n",
    "    '''A basic print function for a KWIC object\n",
    "    \n",
    "    Args:\n",
    "        kwic -- a list of KWIC lines of the form [ [left words], kw, [right words]]\n",
    "        win  -- if None then use all words provided in context otherwise limit by win\n",
    "        \n",
    "    Prints KWIC lines with left context width/padding win*8 characters\n",
    "    '''\n",
    "    \n",
    "    if not kwic:\n",
    "        return\n",
    "    \n",
    "    if win is None:\n",
    "        win = len(kwic[0][0])\n",
    "    \n",
    "    for line in kwic:\n",
    "        print(\"{: >{}}  {}  {}\".format(' '.join(line[0][-win:]), \n",
    "                                      win*10, \n",
    "                                      line[1], \n",
    "                                      ' '.join(line[2][:win])\n",
    "                                     )\n",
    "             )    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6a81a049",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_kwic(kwic, order=None):\n",
    "    ''' sort a kwic list using the passed positional arguments \n",
    "    \n",
    "    Args:\n",
    "        kwic   -- a list of lists [ [left tokens], kw, [right tokens]]\n",
    "        order  -- a list of one or more positional arguments of form side-pos, e.g. L1, R3, L4 (default: None)\n",
    "    \n",
    "    Returns:\n",
    "        kwic sorted for each positional argument in reverse, i.e. ['R1','L1'] sorts first by L1 and then R1\n",
    "    '''\n",
    "    if order is None:\n",
    "        return kwic\n",
    "   \n",
    "    order = [order] if not type(order) is list else order\n",
    "    order.reverse()\n",
    "    \n",
    "    for sort_term in order:\n",
    "        if not re.match('[LR][1-4]', sort_term):\n",
    "            pass\n",
    "        \n",
    "        pos1 = 0 if sort_term[0]=='L' else 2\n",
    "        pos2 = int(sort_term[1])-1\n",
    "        pos2 = 3-pos2 if sort_term[0]=='L' else pos2\n",
    "        kwic.sort(key=lambda l : l[pos1][pos2])\n",
    "    \n",
    "    return kwic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ad295bec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collocates(tokens, kw, win=[4,4]):\n",
    "    '''return the collocates in a window around a given keyword\n",
    "    \n",
    "    Args:\n",
    "          tokens -- a list of tokens\n",
    "          kw     -- keyword string to find and get collocates for\n",
    "          win    -- a list of number of tokens to left (index 0) and right (index 1) to use; default: [4,4]\n",
    "    \n",
    "    Returns:\n",
    "          a list of contexts (matching window specification) around each instance of keyword in tokens\n",
    "    '''\n",
    "    hits = [p for p,t in enumerate(tokens) if t==kw]\n",
    "    \n",
    "    context=[]\n",
    "    for hit in hits:\n",
    "        left = [] if win[0]<1 else tokens[hit-win[0]:hit]\n",
    "        right = [] if win[1]<1 else tokens[hit+1:hit+win[1]+1]\n",
    "        \n",
    "        context.extend(left)\n",
    "        context.extend(right)\n",
    "        \n",
    "    return context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2506d54f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_colls(texts,kw, win=[4,4]):\n",
    "    '''create a collocate frequency list for instances of a kw in a list of texts\n",
    "    \n",
    "    Args:\n",
    "        texts  -- a list of tokenized texts\n",
    "        kw     -- keyword string to find and get collocates for\n",
    "        win    -- a list of number of tokens to left (index 0) and right (index 1) to use; default: [4,4]\n",
    "    \n",
    "    Returns:\n",
    "        a list-of-tuples where each tuple is (collocate, freq_with_kw, coll_total_freq)\n",
    "    '''\n",
    "    word_dist = Counter()\n",
    "    colls = Counter()\n",
    "    for text, tokens in texts.items():\n",
    "        word_dist.update(tokens)\n",
    "        colls.update(collocates(tokens,kw, win))\n",
    "    \n",
    "    return [(str(k),v, word_dist[k]) for k,v in colls.items()], word_dist.get(kw), sum(word_dist.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e52d45e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_freq(freq, size, base=100000):\n",
    "    '''normalize the frequency of an item based on the size of the text/corpus using a base, e.g. per 100,000 words\n",
    "    \n",
    "    Args:\n",
    "        freq   --  the frequency of the item\n",
    "        size   --  the size (number of tokens) in the text/corpus\n",
    "        base   --  normalization unit \n",
    "    \n",
    "    Returns:\n",
    "        normalized frequency\n",
    "    \n",
    "    '''\n",
    "    norm_freq = freq/size * base\n",
    "    return norm_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fb545e7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_items(dist1, size1, dist2, size2, dist3, size3, dist4, size4, dist5, size5, items, scaling=10000, dp=15):\n",
    "    ''' given two Counter objects with common keys compare the frequency and relative frequency of list of items\n",
    "    \n",
    "    Args:\n",
    "        dist1    -- Counter frequency list object\n",
    "        dist2    -- Counter frequency list object\n",
    "        items    -- list of string items that should be keys in dist1 and dist2\n",
    "        scaling  -- normalization factor, e.g. 10,000 words (default: 100000)\n",
    "    \n",
    "    Returns:\n",
    "    \n",
    "        list of tuples of form\n",
    "            (item, item_freq_dist1, norm_item_freq_dist1, item_freq_dist2, norm_item_freq_dist2)\n",
    "    '''\n",
    "    \n",
    "    \n",
    "    item_comparison = []\n",
    "    \n",
    "    for item in items:\n",
    "        \n",
    "        d1_freq = dist1.get(item,0)\n",
    "        d2_freq = dist2.get(item,0)\n",
    "        d3_freq = dist3.get(item,0)\n",
    "        d4_freq = dist4.get(item,0)\n",
    "        d5_freq = dist5.get(item,0)\n",
    "        \n",
    "        item_comparison.append((item, \n",
    "                                d1_freq, round(d1_freq/size1*scaling, dp),\n",
    "                                d2_freq, round(d2_freq/size2*scaling, dp),\n",
    "                                d3_freq, round(d3_freq/size3*scaling, dp),\n",
    "                                d4_freq, round(d4_freq/size4*scaling, dp),\n",
    "                                d5_freq, round(d5_freq/size5*scaling, dp)))\n",
    "    \n",
    "    return item_comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "76294a43",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_item(dist1, dist2,dist3,dist4,dist5, items, scaling=10000, dp=15):\n",
    "    ''' given two Counter objects with common keys compare the frequency and relative frequency of list of items\n",
    "    \n",
    "    Args:\n",
    "        dist1    -- Counter frequency list object\n",
    "        dist2    -- Counter frequency list object\n",
    "        items    -- list of string items that should be keys in dist1 and dist2\n",
    "        scaling  -- normalization factor, e.g. 10,000 words (default: 100000)\n",
    "    \n",
    "    Returns:\n",
    "    \n",
    "        list of tuples of form\n",
    "            (item, item_freq_dist1, norm_item_freq_dist1, item_freq_dist2, norm_item_freq_dist2)\n",
    "    '''\n",
    "    \n",
    "    size1 = sum(dist1.values())\n",
    "    size2 = sum(dist2.values())\n",
    "    size3 = sum(dist3.values())\n",
    "    size4 = sum(dist4.values())\n",
    "    size5 = sum(dist5.values())\n",
    "    \n",
    "    \n",
    "    item_comparison = []\n",
    "    \n",
    "    for item in items:\n",
    "        \n",
    "        d1_freq = dist1.get(item,0)\n",
    "        d2_freq = dist2.get(item,0)\n",
    "        d3_freq = dist3.get(item,0)\n",
    "        d4_freq = dist4.get(item,0)\n",
    "        d5_freq = dist5.get(item,0)\n",
    "        \n",
    "        item_comparison.append((item, \n",
    "                                round(d1_freq/size1*scaling, dp),\n",
    "                                round(d2_freq/size2*scaling, dp),\n",
    "                                round(d3_freq/size3*scaling, dp),\n",
    "                                round(d4_freq/size4*scaling, dp),\n",
    "                                round(d5_freq/size5*scaling, dp)))\n",
    "    \n",
    "    return item_comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3a3fb0fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_keyitems(df, n=20, c1='red', c2='blue', corpusA='corpus A', corpusB='corpus B'):\n",
    "    '''plot  top/bottom n items from a keyness analysis table\n",
    "    \n",
    "    Args:\n",
    "        df - a data frame created by calculated_keyness with cols: item, keyness\n",
    "        num - the number of top and bottom ranked items to include\n",
    "        c1/c2 - color for the bars\n",
    "    \n",
    "    Returns:\n",
    "        HTML string containing two column table\n",
    "    '''\n",
    "   \n",
    "    template = '''\n",
    "        <div style=' float:left; width: 40%; text-align: center'>\n",
    "        <h3>{}</h3>\n",
    "        {}</div>\n",
    "       <div style='width: 40%; padding-left: 20px; float: left; '>\n",
    "       <h3 style=\"text-align: center\">{}</h3>\n",
    "        {}</div>\n",
    "    '''\n",
    "\n",
    "\n",
    "    idiv = '''\n",
    "            <div style=\"font-size: {}px; color: {}; margin-bottom: 2px; float: left; \n",
    "            margin: 10px; padding: 2px; background-color: #f7f7f7; border-radius: 6px\">\n",
    "            {}</div>\n",
    "            '''\n",
    "    \n",
    "    top = df[['item', 'keyness']].head(n).values\n",
    "    bottom = df[['item', 'keyness']].tail(n).values\n",
    "\n",
    "    top_str = '\\n'.join([idiv.format(3*math.log(kness), c1, item) for size, (item, kness) in enumerate(top,1)])\n",
    "    bottom_str = '\\n'.join([idiv.format(3*math.log(abs(kness)), c2, item) for size, (item, kness) in enumerate(bottom,1)])\n",
    "    \n",
    "    \n",
    "    display(HTML(\n",
    "        template.format(corpusA,top_str, corpusB, bottom_str)\n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0f48ea5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def comparison_plot(comparison_data, label1='corpus 1', label2='corpus 2', label3='corpus 3', label4='corpus 4', label5='corpus 5'):\n",
    "    ''' create a paired barplot of relative frequencies of items in two corpora\n",
    "    \n",
    "    Args:\n",
    "        comparison_data --  list of tuples produced by the compare_items() function\n",
    "        label1          --  legend label for first corpus (default: corpus 1)\n",
    "        label2          --  legend label for second corpus (default: corpus 2)\n",
    "        \n",
    "    Produces a Seaborn barplot\n",
    "    '''\n",
    "    fig = plt.figure(figsize=(12,6))\n",
    "    \n",
    "    df=pd.DataFrame(comparison_data)[[0,2,4,6,8,10]] \n",
    "    df.columns = ['item', label1, label2, label3, label4, label5]\n",
    "    df2=df.melt(id_vars=['item'])\n",
    "    df2.columns=['item', 'corpus', 'frequency']\n",
    "    sn.barplot(x='item',y='frequency', hue='corpus',data=df2, palette = 'OrRd')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fd9fff93",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_plot(comparison_data, label1='corpus 1', label2='corpus 2', label3='corpus 3', label4='corpus 4', label5='corpus 5'):\n",
    "    ''' create a paired barplot of relative frequencies of items in two corpora\n",
    "    \n",
    "    Args:\n",
    "        comparison_data --  list of tuples produced by the compare_items() function\n",
    "        label1          --  legend label for first corpus (default: corpus 1)\n",
    "        label2          --  legend label for second corpus (default: corpus 2)\n",
    "        \n",
    "    Produces a Seaborn barplot\n",
    "    '''\n",
    "    fig = plt.figure(figsize=(12,6))\n",
    "    \n",
    "    df=pd.DataFrame(comparison_data)[[0,1,2,3,4,5]] \n",
    "    df.columns = ['item', label1, label2, label3, label4, label5]\n",
    "    df2=df.melt(id_vars=['item'])\n",
    "    df2.columns=['item', 'corpus', 'frequency']\n",
    "    sn.barplot(x='item',y='frequency', hue='corpus',data=df2, palette = 'OrRd')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5d000e44",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_collocates(kw, collocate_list, num=20, show_freq=False, title=None, threshold=1):\n",
    "    ''' Create a graph of the collocates of a keyword within a specified window and threshold\n",
    "    \n",
    "    Args:\n",
    "        kw              -- keyword to place at center of graph\n",
    "        collocate_list  -- Counter object of collocate frequencies\n",
    "        num             -- the number of collocates (in descending frequency to display) [default=20]\n",
    "        show_freq       -- whether to show frequency beside edge True/False [default=False]\n",
    "        title           -- string to use as a title for the plot [default=None]\n",
    "        threshold       -- frequency threshold for showing edges [default=1]\n",
    "        \n",
    "    '''\n",
    "    cG = graphviz.Graph(engine='neato')\n",
    "    cG.attr('graph', overlap='scalexy', size=\"6,6\")\n",
    "    if title:\n",
    "        cG.attr('graph', label=title, labelloc='t', fontsize='20')\n",
    "    for item, freq in collocate_list.most_common(num):\n",
    "        if freq >= threshold:\n",
    "            cG.edge(kw.upper(), item, penwidth=str(math.log(freq,2)), \n",
    "                    label=None if not show_freq else str(freq))\n",
    "    \n",
    "    return cG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "21cfcdef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pmi(A, B, AB, N, dp=2):\n",
    "    '''calculate pointwise mutual information for a pair of words given their co-occurring frequency and total frequencies\n",
    "    \n",
    "    Args:\n",
    "        A   -- total frequency of word 1\n",
    "        B   -- total frequency of word 2\n",
    "        AB  -- frequency of word 1 and word 2 together\n",
    "        N   -- number of tokens in corpus/sample\n",
    "        \n",
    "    Returns:\n",
    "        the PMI value   log2( AB / A*B * N)\n",
    "    '''\n",
    "    return round(math.log2(N* (AB / (A * B))),dp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9e6a4b45",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_tscore(combined_freq, node_freq, coll_freq,  corpus_size, dp=2):\n",
    "    sample_mean = combined_freq / corpus_size\n",
    "    independence = (node_freq/corpus_size) * (coll_freq/corpus_size)\n",
    "    \n",
    "    top = sample_mean - independence\n",
    "    bottom = math.sqrt(sample_mean/corpus_size)\n",
    "    \n",
    "    return round(top/bottom,dp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3fd4d216",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_colls(collocate, tokens):\n",
    "    coll_colls=Counter()\n",
    "    coll_colls.update(collocates(tokens, collocate, win=[4,4]))\n",
    "    token_dist = Counter(tokens)\n",
    "    corpus_size = sum(token_dist.values())\n",
    "    collocate_freq = token_dist[collocate]\n",
    "    row=[]\n",
    "    for item, freq in coll_colls.most_common(500):\n",
    "        item_freq = token_dist[item]\n",
    "        pmi_val = pmi(collocate_freq, item_freq, freq, corpus_size)\n",
    "        tscore = calculate_tscore(freq, collocate_freq, item_freq, corpus_size)\n",
    "\n",
    "        #print(f'{collocate} - {item} ({freq})  < {item_freq} {pv1}  {pmi_val} {tscore}')\n",
    "\n",
    "        row.append({\n",
    "            'coll': item,\n",
    "            'joint_freq': freq,\n",
    "            'pmi': pmi_val,\n",
    "            'tscore': tscore\n",
    "        })    \n",
    "\n",
    "    coll_df = pd.DataFrame(row)\n",
    "    return coll_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c17425be-41cc-4f55-8312-68fc398c6067",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_coll_network(node, corpus, n=30, measure='pmi', \n",
    "                      DEBUG=False, exclude=None, min_freq=10,\n",
    "                      add_communities=False, stoplist=False):\n",
    "    \n",
    "    colls_df = calculate_colls(node, corpus)\n",
    "    \n",
    "    if stoplist:\n",
    "        stoplist_EN = stopwords.raw('english').split('\\n')\n",
    "        \n",
    "        colls_df = colls_df[-colls_df['coll'].isin(stoplist_EN)]\n",
    "    \n",
    "    freq_filter = colls_df['joint_freq']>=min_freq\n",
    "    \n",
    "    top_colls_df=colls_df[freq_filter].sort_values(measure, ascending=False).head(n)\n",
    "    \n",
    "    \n",
    "    cG=nx.DiGraph()\n",
    "\n",
    "    if exclude is None:\n",
    "        exclude = [node]\n",
    "    else:\n",
    "        exclude.append(node)\n",
    "    \n",
    "    res=top_colls_df.apply(lambda r: cG.add_edge(node.upper(),r['coll']), axis=1)\n",
    "    \n",
    "    for coll in top_colls_df['coll']:\n",
    "        if DEBUG:\n",
    "            print(f'Getting collocates for {coll}')\n",
    "        coll_colls_df = calculate_colls(coll, corpus)\n",
    "        shared=coll_colls_df['coll'].isin(top_colls_df['coll'])\n",
    "        freq_filter = coll_colls_df['joint_freq']>=min_freq/2\n",
    "        shared_colls = coll_colls_df[shared & freq_filter].sort_values(measure).head(n*2)\n",
    "\n",
    "\n",
    "\n",
    "        if shared_colls.shape[0]>0:\n",
    "            if DEBUG:\n",
    "                print(shared_colls)\n",
    "            for scoll in shared_colls['coll']:\n",
    "                if coll in exclude:\n",
    "                    continue\n",
    "                if coll!=scoll and scoll not in exclude:\n",
    "                    cG.add_edge(coll,scoll)  \n",
    "                    \n",
    "    if add_communities:\n",
    "        c = community.greedy_modularity_communities(cG)\n",
    "        for gnum, cm in enumerate(c,1):\n",
    "            for n in cm:\n",
    "                if n!=node.upper():\n",
    "                    cG.nodes[n]['group']=gnum\n",
    "\n",
    "    return cG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "baf1d956-90c9-4855-b171-ad4a227660c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_all(tokens, seq):\n",
    "    hits=[]\n",
    "    for i,token in enumerate(tokens):\n",
    "        if token==seq:\n",
    "            hits.append(i)\n",
    "    return hits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a4e873a6-a670-443f-9e2a-bd6330bc6bb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dispersion_plot(hits, title=None):\n",
    "    plt.figure(figsize=(12,2))\n",
    "    plt.vlines(hits, ymin=0, ymax=1, alpha=0.2)\n",
    "    plt.xlabel('Position in text (tokens)')\n",
    "    plt.yticks([])\n",
    "    if title:\n",
    "        plt.title(title)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "de459b74-ae73-439e-8231-5b42dbb30b7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dispersion_plot_overlays(hits_dict, title=None):\n",
    "    plt.figure(figsize=(12, 4))  # Increased height to accommodate staggering\n",
    "    colors = plt.cm.tab10(np.linspace(0, 1, len(hits_dict)))  # Colormap\n",
    "    \n",
    "    # Calculate vertical positions to stagger the lines\n",
    "    n_categories = len(hits_dict)\n",
    "    vertical_positions = np.linspace(0, 1, n_categories)\n",
    "    \n",
    "    for i, (category, hits) in enumerate(hits_dict.items()):\n",
    "        # Plot vertical lines staggered vertically\n",
    "        plt.vlines(hits, \n",
    "                   ymin=vertical_positions[i] - 0.1,  # Slightly lower than the position\n",
    "                   ymax=vertical_positions[i] + 0.1,  # Slightly higher than the position\n",
    "                   color=colors[i], \n",
    "                   label=category, \n",
    "                   alpha=0.8) \n",
    "        \n",
    "    plt.xlabel('Position in text (tokens)')\n",
    "    plt.yticks(vertical_positions, list(hits_dict.keys()))  # Use category names as y-tick labels\n",
    "    plt.legend(loc='upper right', bbox_to_anchor=(1.1, 1), title=\"Word Categories\")\n",
    "    plt.title(title)\n",
    "    plt.tight_layout()  # Adjust layout to prevent cutting off labels\n",
    "    plt.show()\n",
    "    \n",
    "    # Print word counts\n",
    "    for category, hits in hits_dict.items():\n",
    "        count = len(hits)\n",
    "        print(category + ' Words Count:', count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5ad87b5-392f-4c17-8a96-00c0df5239c8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "citation-manager": {
   "items": {}
  },
  "kernelspec": {
   "display_name": "Python 3.8",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
